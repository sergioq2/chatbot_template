{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from PyPDF2 import PdfReader\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_text(pdf_doc):\n",
    "    text = []\n",
    "    pdf_reader = PdfReader(pdf_doc)\n",
    "    for page_number, page in enumerate(pdf_reader.pages):\n",
    "        text.append((page_number + 1, page.extract_text()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_pdf(data, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\"],\n",
    "        length_function=len)\n",
    "    text_chunks = text_splitter.create_documents([data])\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "def process_pd_documents(path):\n",
    "    for file_index, file in enumerate(os.listdir(path)):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            try:\n",
    "                pages = get_pdf_text(os.path.join(path, file))\n",
    "            except:\n",
    "                continue\n",
    "            title = file.replace('.pdf', '')\n",
    "            for page_number, page_text in pages:\n",
    "                page_text = page_text.replace(\".\", \"\")\n",
    "                chunks_parent = split_text_pdf(page_text, 1500, 300)\n",
    "                for chunk_index, chunk in enumerate(chunks_parent):\n",
    "                    parent_text = chunk.page_content\n",
    "                    child_chunks = split_text_pdf(parent_text, 200, 100)\n",
    "                    for child_chunk in child_chunks:\n",
    "                        entry = {\n",
    "                            'title': title,\n",
    "                            'page_number': page_number,\n",
    "                            'text_parent': parent_text,\n",
    "                            'text_child': child_chunk.page_content\n",
    "                        }\n",
    "                        texts.append(entry)\n",
    "    df = pd.DataFrame(texts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../aerocivil_docs/'\n",
    "df = process_pd_documents(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    df = df.fillna('.')\n",
    "    df = df.replace('\\n',' ', regex=True)\n",
    "    df = df.replace('#',' ', regex=True)\n",
    "    df = df.replace('-',' ', regex=True)\n",
    "    df = df.replace('__',' ', regex=True)\n",
    "    df = df.replace('@',' ', regex=True)\n",
    "    df = df.replace('/',' ', regex=True)\n",
    "    df = df.replace('`',' ', regex=True)\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    df = df.replace('>',' ', regex=True)\n",
    "    df = df.replace('<',' ', regex=True)\n",
    "    df = df.replace('{',' ', regex=True)\n",
    "    df = df.replace('}',' ', regex=True)\n",
    "    df = df.replace('!',' ', regex=True)\n",
    "    df = df.replace('[^\\w\\s]','', regex=True)\n",
    "    df = df.replace('pdf','', regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_to_doc = {title: f'doc{i+1}' for i, title in enumerate(df['title'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc'] = df['title'].map(title_to_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['title']\n",
    "df['doi'] = '1102'\n",
    "df['chunk-id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', \n",
    "                              region_name='us-east-1')\n",
    "embeddings_bedrock = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
    "                                       client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_key = os.environ.get(\"PINECONE_KEY\", \"default_endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=\"aerocivildocs\",\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-west-2\"\n",
    "    ) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(\"aerocivildocs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "batch_size = 250\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size)):\n",
    "    i_end = min(len(df), i+batch_size)\n",
    "    batch = df.iloc[i:i_end]\n",
    "    ids = [f\"{x['doc']}#{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    texts = [x['text_child'] for _, x in batch.iterrows()]\n",
    "    embeds = embeddings_bedrock.embed_documents(texts)\n",
    "    metadata = [\n",
    "        {'text': x['text_parent'],\n",
    "         'title': x['title'],\n",
    "         'page_number': x['page_number'],\n",
    "          'id': x['id']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata), namespace='aero')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
